# -*- coding: utf-8 -*-
"""railbot (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wyk7hHWGye4CeAuD3DICUPNWPG_baMjp
"""

!git clone https://github.com/kailashjagadeesh/DL_FasteNet.git

# Commented out IPython magic to ensure Python compatibility.
# %cd DL_FasteNet/

!python main.py

!pip install pthflops

#!/usr/bin/env python3
import time
import os
import sys

import cv2
import numpy as np
from numpy import random as nprand
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms.functional as TF

from helpers import helpers
from ImageLoader import ImageLoader
from FasteNet_Net_v2 import FasteNet_v2

from FasteNet_Vanilla_Net import FasteNet_Vanilla
from FasteNet_Large_Net import FasteNet_Large

# params
DIRECTORY = 'C:\AI\DATA' # os.path.dirname(__file__)
DIRECTORY2 = '/content/DL_FasteNet'
SHUTDOWN_AFTER_TRAINING = False

VERSION_NUMBER = 5
MARK_NUMBER = 505

BATCH_SIZE = 100
NUMBER_OF_IMAGES = 700
NUMBER_OF_CYCLES = 5

# instantiate helper object
# helpers = helpers(mark_number=MARK_NUMBER, version_number=VERSION_NUMBER, weights_location=DIRECTORY2)
# device = helpers.get_device()


# uncomment this to one to view the output of the dataset
# helpers.peek_dataset(dataloader=dataloader)

# set up net
FasteNet = FasteNet_v2().to(device)

# get latest weight file
weights_file = helpers.get_latest_weight_file()
# weights_file  = "/content/DL_FasteNet/weights/Version7/weights54.pth"
if weights_file != -1:
    FasteNet.load_state_dict(torch.load(weights_file))

# set up loss function and optimizer and load in data
loss_function = nn.MSELoss()
optimizer = optim.Adam(FasteNet.parameters(), lr=1e-6, weight_decay=1e-2)

# get network param number and gflops
# image_path = os.path.join(DIRECTORY, f'Dataset/image/image_{1}.png')
# image = TF.to_tensor(cv2.imread(image_path))[0].unsqueeze(0).unsqueeze(0)[..., :1600].to(device)
# image /= torch.max(image + 1e-6)
# helpers.network_stats(FasteNet, image)


# HARD NEGATIVE MINING
# HARD NEGATIVE MINING
# HARD NEGATIVE MINING

# FOR INFERENCING
# FOR INFERENCING
# FOR INFERENCING

# set frames to render > 0 to perform inference

import cv2
img=cv2.imread(r'/content/1.jpeg',0)
print(img.shape)
img=cv2.resize(img,(512,1600))
cv2.imwrite("/content/check.jpeg",img)

# img= img.reshape(1,1600,512)
print(img.shape)

####################################################################################
# import torchvision.transforms.functional as TF
# img=cv2.imread(r'/content/drive/MyDrive/rgb/1608798358_0.png',0)
# img=cv2.resize(img,(512,1600))
# img = TF.to_tensor(img)

# print(img.shape)

from google.colab import drive
drive.mount('/content/drive')

torch.no_grad()
FasteNet.eval()
frames_to_render = 1
start_time = time.time()
#out = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'DIVX'),15)

# set to true for inference
for _ in range(frames_to_render):
    
    # read images
    image = TF.to_tensor(img)[0]
    # print(image.shape)
    
    # normalize inputs, 1e-6 for stability as some images don't have truth masks (no fasteners)
    image /= torch.max(image + 1e-6)
    
    input = image.unsqueeze(0).unsqueeze(0).to(device)[..., :1600]
    # input = image
    print(input.shape)
    saliency_map = FasteNet.forward(input)
    torch.cuda.synchronize()
    # print(saliency_map)

    # draw contours on original image and prediction image
    contour_image, contour_number = helpers.saliency_to_contour(input=saliency_map, original_image=input, fastener_area_threshold=0, input_output_ratio=8)
    print(contour_image.shape)
    # use this however you want to use it
    image_image = np.array(cv2.resize(cv2.imread(r'/content/1.jpeg',0),(512,1600)), dtype=np.float64)
    print(image_image.shape)

    image_image /= 205 # approximate multiplier, I don't actually know the scale anymore at this point
    inter_img = np.array([contour_image, image_image])
    # print(inter_img)
    # fused_image = np.transpose(np.array([contour_image, image_image]), [2,1,0])
    # fused_image = np.array([contour_image, image_image])
    fused_image = np.ones((1600, 512))
    
    for i in range(1600):
      for j in range(512):
        fused_image[i][j] = (contour_image[i][j] + image_image[i][j])/2
    
    cv2.imwrite("/content/check.jpeg",contour_image)

    # print(fused_image)

    # set to true to display images
    if True:
        figure = plt.figure()

        figure.add_subplot(2, 2, 1)
        plt.title(f'Input Image: Index {index}')
        plt.imshow(input.squeeze().to('cpu').detach().numpy(), cmap='gray')
        figure.add_subplot(2, 2, 2)
        plt.title('Saliency Map')
        plt.imshow(saliency_map.squeeze().to('cpu').detach().numpy(), cmap='gray')
        figure.add_subplot(2, 2, 3)
        plt.title('Predictions')
        plt.imshow(fused_image)
        plt.title(f'Predicted Number of Fasteners in Image: {contour_number}')
        
        plt.show()

end_time = time.time()
duration = end_time - start_time
print(f"Average FPS = {frames_to_render / duration}")



